# app08.py
# ============================================================
# ğŸ”® Q-Quest é‡å­ç¥è¨— app08ï¼ˆå®Œæˆç‰ˆï¼‰
# - çµ±åˆExcelï¼ˆpackï¼‰ã‚’æœ€å„ªå…ˆã§èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼‰
# - èª“é¡˜ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ + ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ› â†’ VOWãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆSENSEè¾æ›¸ + char n-gramè£œåŠ©ï¼‰
# - QUBOï¼ˆ12å¤‰æ•°ï¼‰ã‚’ SAï¼ˆç„¼ããªã¾ã—ï¼‰ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â†’ è¦³æ¸¬åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆï¼‰
# - ã€Œä»Šå›è¦³æ¸¬ã•ã‚ŒãŸç¥ã€ï¼‹ã€Œå…¥åŠ›ã®å½±éŸ¿ï¼ˆå¯„ä¸ã®å¯è¦–åŒ–ï¼‰ã€ï¼‹ã€Œç¥è¨—æ–‡ï¼ˆæ ¼è¨€/é‡‘è¨€ä»˜ãï¼‰ã€
#
# æœŸå¾…ã™ã‚‹çµ±åˆExcelã®ä¸»ã‚·ãƒ¼ãƒˆåï¼ˆåŒåãŒæœ›ã¾ã—ã„ï¼‰ï¼š
#   VOW_DICT, CHAR_TO_VOW, CHAR_MASTER, SENSE_DICT, SENSE_TO_VOW, (ä»»æ„: QUOTES)
#
# æ—¢çŸ¥ã®åˆ—åï¼ˆã‚ãªãŸã®çµ±åˆExcelã«åˆã‚ã›ã¦å¸åï¼‰ï¼š
#   VOW_DICT: VOW_ID, LABEL, TITLE, SUBTITLE, DESCRIPTION_LONG, UI_HINT ...
#   CHAR_TO_VOW: CHAR_ID, IMAGE_FILE, å…¬å¼ã‚­ãƒ£ãƒ©å, VOW_01..VOW_12
#   CHAR_MASTER: CHAR_ID, å…¬å¼ã‚­ãƒ£ãƒ©å, å½¹å‰², å½¹å‰²è£œè¶³èª¬æ˜, çµµé¦¬æ–‡å­—åˆ†æ, VOW_01..12, AXIS_*
#   SENSE_TO_VOW: (ä¾‹) SENSE, VOW_ID, WEIGHT ãªã©ï¼ˆå¤šå°‘ã‚†ã‚‰ã„ã§ã‚‚å¸åï¼‰
#   QUOTES: TEXT / QUOTE / æ ¼è¨€ / BODY ãªã©ï¼ˆå¤šå°‘ã‚†ã‚‰ã„ã§ã‚‚å¸åï¼‰
#
# ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ï¼š
#   ./assets/images/characters/ ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ¨å¥¨ï¼‰
# ============================================================

import os
import re
import math
import random
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st
from PIL import Image


# -----------------------------
# åŸºæœ¬è¨­å®š
# -----------------------------
st.set_page_config(page_title="ğŸ”® Q-Quest é‡å­ç¥è¨— app08", layout="wide")


# -----------------------------
# å°é“å…·
# -----------------------------
def _norm01(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x, dtype=float)
    mn, mx = float(np.min(x)), float(np.max(x))
    if mx - mn < 1e-12:
        return np.zeros_like(x)
    return (x - mn) / (mx - mn)

def _safe_float(x, default=0.0) -> float:
    try:
        if pd.isna(x):
            return default
        return float(x)
    except Exception:
        return default

def _first_existing(col_candidates: List[str], cols: List[str]) -> Optional[str]:
    cols_set = set(cols)
    for c in col_candidates:
        if c in cols_set:
            return c
    return None

def _ensure_vow_cols(df: pd.DataFrame, n_vow: int = 12) -> List[str]:
    """VOW_01..VOW_12 ã®åˆ—åã‚’æ¢ã—ã¦è¿”ã™ï¼ˆç„¡ã‘ã‚Œã°ã‚¨ãƒ©ãƒ¼ï¼‰"""
    needed = [f"VOW_{i:02d}" for i in range(1, n_vow + 1)]
    cols = list(df.columns)
    missing = [c for c in needed if c not in cols]
    if missing:
        raise ValueError(f"VOWåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing}")
    return needed

def _read_image_maybe(path: str) -> Optional[Image.Image]:
    try:
        if path and os.path.exists(path):
            return Image.open(path)
    except Exception:
        return None
    return None

def _softmax(logits: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    t = max(1e-6, float(temperature))
    z = logits / t
    z = z - np.max(z)
    e = np.exp(z)
    s = e / (np.sum(e) + 1e-12)
    return s

def _tokenize_jp_loose(text: str) -> List[str]:
    """å½¢æ…‹ç´ ãªã—ã®â€œã‚†ã‚‹ã„â€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæ—¥æœ¬èª/è‹±èªæ··åœ¨ã§ã‚‚OKï¼‰"""
    if not text:
        return []
    t = text.strip()
    t = re.sub(r"\s+", " ", t)
    # è¨˜å·ã§åŒºåˆ‡ã‚‹
    parts = re.split(r"[ \t\n\r,ï¼Œã€ã€‚ï¼.!ï¼?ï¼Ÿ:ï¼š;ï¼›/ï¼()\[\]{}ã€Œã€ã€ã€â€œâ€\"'`~\-_=+<>ï¼œï¼]+", t)
    return [p for p in parts if p]

def _char_ngrams(text: str, n: int = 3) -> List[str]:
    t = re.sub(r"\s+", "", (text or ""))
    if len(t) < n:
        return [t] if t else []
    return [t[i:i+n] for i in range(len(t) - n + 1)]


# -----------------------------
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ï¼ˆçµ±åˆExcelå„ªå…ˆï¼‰
# -----------------------------
@dataclass
class PackData:
    sheets: Dict[str, pd.DataFrame]
    vow_dict: pd.DataFrame
    char_to_vow: pd.DataFrame
    char_master: pd.DataFrame
    sense_dict: Optional[pd.DataFrame]
    sense_to_vow: Optional[pd.DataFrame]
    quotes: Optional[pd.DataFrame]
    sheet_names: List[str]

def load_pack_excel(file) -> PackData:
    # pandasã® dict(DataFrame) ã¯æ™®é€šã«pickleå¯èƒ½ã§ã™ãŒã€ç’°å¢ƒå·®ã§ st.cache_data ãŒã‚³ã‚±ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€ã‚ãˆã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã›ã‚“ã€‚
    xls = pd.read_excel(file, sheet_name=None, engine="openpyxl")
    sheets = {str(k): v.copy() for k, v in xls.items()}
    sheet_names = list(sheets.keys())

    # å¿…é ˆã‚·ãƒ¼ãƒˆ
    if "VOW_DICT" not in sheets:
        raise ValueError("çµ±åˆExcelã« VOW_DICT ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    if "CHAR_TO_VOW" not in sheets:
        raise ValueError("çµ±åˆExcelã« CHAR_TO_VOW ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    if "CHAR_MASTER" not in sheets:
        raise ValueError("çµ±åˆExcelã« CHAR_MASTER ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    vow_dict = sheets["VOW_DICT"]
    char_to_vow = sheets["CHAR_TO_VOW"]
    char_master = sheets["CHAR_MASTER"]

    sense_dict = sheets.get("SENSE_DICT", None)
    sense_to_vow = sheets.get("SENSE_TO_VOW", None)
    quotes = sheets.get("QUOTES", None)  # ã¾ã¨ã‚ãŸãªã‚‰ã“ã“ã«å…¥ã£ã¦ã„ã‚‹æƒ³å®š

    return PackData(
        sheets=sheets,
        vow_dict=vow_dict,
        char_to_vow=char_to_vow,
        char_master=char_master,
        sense_dict=sense_dict,
        sense_to_vow=sense_to_vow,
        quotes=quotes,
        sheet_names=sheet_names,
    )


# -----------------------------
# SENSEâ†’VOW ãƒãƒƒãƒ—æ§‹ç¯‰ï¼ˆåˆ—åã‚†ã‚‰ãå¸åï¼‰
# -----------------------------
def build_sense_maps(sense_dict: Optional[pd.DataFrame], sense_to_vow: Optional[pd.DataFrame]) -> Tuple[Dict[str, str], Dict[str, np.ndarray]]:
    """
    è¿”ã‚Šå€¤:
      sense_label_map: æ­£è¦åŒ–ã—ãŸ sense_key -> è¡¨ç¤ºãƒ©ãƒ™ãƒ«
      sense2vow_vec:   æ­£è¦åŒ–ã—ãŸ sense_key -> vow_vec(12)
    """
    sense_label_map: Dict[str, str] = {}
    sense2vow_vec: Dict[str, np.ndarray] = {}

    if sense_to_vow is None or len(sense_to_vow) == 0:
        return sense_label_map, sense2vow_vec

    df = sense_to_vow.copy()
    cols = list(df.columns)

    # senseåˆ—å€™è£œ
    sense_col = _first_existing(
        ["SENSE", "SENSE_KEY", "SENSE_ID", "KEY", "WORD", "TERM", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "è¨€è‘‰", "æ¦‚å¿µ"],
        cols,
    )
    # vowåˆ—å€™è£œï¼ˆVOW_IDãŒç†æƒ³ã ãŒã€VOW_01..12 ç›´åˆ—ã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹ï¼‰
    vow_id_col = _first_existing(["VOW_ID", "VOW", "VOW_KEY", "èª“é¡˜ID", "èª“é¡˜"], cols)
    weight_col = _first_existing(["WEIGHT", "W", "SCORE", "é‡ã¿", "å¯„ä¸"], cols)

    # Case A: è¡Œå½¢å¼ï¼ˆsense, vow_id, weightï¼‰
    if sense_col and vow_id_col:
        # VOW_IDãŒ "VOW_01" å½¢å¼ / "01" / 1 ãªã©æºã‚Œã‚‹ã®ã§å¸å
        def vow_index(v):
            if pd.isna(v):
                return None
            s = str(v).strip()
            m = re.search(r"(\d+)", s)
            if not m:
                return None
            k = int(m.group(1))
            if 1 <= k <= 12:
                return k - 1
            return None

        for _, r in df.iterrows():
            sk = str(r.get(sense_col, "")).strip()
            if not sk:
                continue
            key = sk.lower()
            idx = vow_index(r.get(vow_id_col))
            if idx is None:
                continue
            w = _safe_float(r.get(weight_col), default=1.0) if weight_col else 1.0
            if key not in sense2vow_vec:
                sense2vow_vec[key] = np.zeros(12, dtype=float)
            sense2vow_vec[key][idx] += w

        # ãƒ©ãƒ™ãƒ«ï¼ˆSENSE_DICTãŒã‚ã‚Œã°å„ªå…ˆï¼‰
        if sense_dict is not None and len(sense_dict) > 0:
            sdc = list(sense_dict.columns)
            key_col = _first_existing(["SENSE", "SENSE_KEY", "KEY", "SENSE_ID", "ID", "æ¦‚å¿µ"], sdc)
            label_col = _first_existing(["LABEL", "NAME", "è¡¨ç¤ºå", "åç§°", "ãƒ©ãƒ™ãƒ«"], sdc)
            if key_col:
                for _, r in sense_dict.iterrows():
                    sk = str(r.get(key_col, "")).strip()
                    if not sk:
                        continue
                    sense_label_map[sk.lower()] = str(r.get(label_col, sk)).strip() if label_col else sk

        # sense_dictãŒç„¡ã„ãªã‚‰è‡ªå‰
        for k in list(sense2vow_vec.keys()):
            if k not in sense_label_map:
                sense_label_map[k] = k

        return sense_label_map, sense2vow_vec

    # Case B: åˆ—å½¢å¼ï¼ˆsenseåˆ— + VOW_01..12åˆ—ï¼‰
    if sense_col:
        try:
            vow_cols = _ensure_vow_cols(df, 12)
            for _, r in df.iterrows():
                sk = str(r.get(sense_col, "")).strip()
                if not sk:
                    continue
                key = sk.lower()
                vec = np.array([_safe_float(r.get(c), 0.0) for c in vow_cols], dtype=float)
                if np.allclose(vec, 0):
                    continue
                sense2vow_vec[key] = vec
                sense_label_map[key] = key
            return sense_label_map, sense2vow_vec
        except Exception:
            return sense_label_map, sense2vow_vec

    return sense_label_map, sense2vow_vec


# -----------------------------
# ãƒ†ã‚­ã‚¹ãƒˆ â†’ VOW ãƒ™ã‚¯ãƒˆãƒ«
# -----------------------------
def text_to_vow_vector(
    text: str,
    sense_label_map: Dict[str, str],
    sense2vow_vec: Dict[str, np.ndarray],
    ngram_n: int = 3,
) -> Tuple[np.ndarray, List[Tuple[str, float]]]:
    """
    è¿”ã‚Šå€¤:
      v_text: 12æ¬¡å…ƒ
      hits:   [(sense_label, hit_score)] ã–ã£ãã‚Šèª¬æ˜ç”¨
    """
    if not text:
        return np.zeros(12, dtype=float), []

    t = text.strip().lower()
    if not t:
        return np.zeros(12, dtype=float), []

    # 1) ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå˜èªï¼‰ä¸€è‡´
    tokens = _tokenize_jp_loose(text)
    tokens_l = [x.lower() for x in tokens]

    # 2) char n-gram ä¸€è‡´ï¼ˆæ—¥æœ¬èªã®ã€Œéƒ¨åˆ†ä¸€è‡´ã€è£œåŠ©ï¼‰
    ngrams = _char_ngrams(t, n=ngram_n)
    ngrams_set = set([g.lower() for g in ngrams if g])

    v = np.zeros(12, dtype=float)
    hits: List[Tuple[str, float]] = []

    # senseã‚­ãƒ¼é›†åˆ
    sense_keys = list(sense2vow_vec.keys())
    for sk in sense_keys:
        key = sk.lower()
        # ã–ã£ãã‚Šã‚¹ã‚³ã‚¢ï¼ˆå˜èªå®Œå…¨ä¸€è‡´ > éƒ¨åˆ†ä¸€è‡´ï¼‰
        score = 0.0
        if key in tokens_l:
            score += 2.0
        # éƒ¨åˆ†ä¸€è‡´ï¼ˆngramå´ï¼‰
        if key and (key in ngrams_set):
            score += 1.0
        # æ–‡å­—åˆ—åŒ…å«ï¼ˆä¿é™ºï¼‰
        if key and (key in t):
            score += 0.5

        if score > 0:
            v += score * sense2vow_vec[sk]
            hits.append((sense_label_map.get(sk, sk), float(score)))

    # æ­£è¦åŒ–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯æš´ã‚Œã‚„ã™ã„ã®ã§è»½ãï¼‰
    if np.linalg.norm(v) > 1e-12:
        v = v / (np.linalg.norm(v) + 1e-12)

    # ãƒ’ãƒƒãƒˆä¸Šä½ã‚’è¿”ã™
    hits.sort(key=lambda x: x[1], reverse=True)
    return v, hits[:12]


# -----------------------------
# QUBOï¼ˆ12å¤‰æ•°ï¼‰ + SAã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
# -----------------------------
@dataclass
class QuboModel:
    h: np.ndarray          # (12,)
    J: np.ndarray          # (12,12) ä¸Šä¸‰è§’ã‚’ä½¿ã†
    lam: float             # åˆ¶ç´„ãƒšãƒŠãƒ«ãƒ†ã‚£
    target_k: int          # é¸æŠæ•°ã®ç›®æ¨™ï¼ˆåŸºæœ¬1ï¼‰

def build_qubo(
    W_char_vow: np.ndarray,    # (12,12)
    v_total: np.ndarray,       # (12,)
    beta_pair: float = 0.15,
    lam: float = 3.0,
    target_k: int = 1,
) -> QuboModel:
    """
    E(x) = sum_i h_i x_i + sum_{i<j} J_ij x_i x_j + lam*(sum x - target_k)^2
    """
    # ä¸€æ¬¡é …ï¼šèª“é¡˜ã¨ã®æ•´åˆï¼ˆä½ã„ã»ã©é¸ã°ã‚Œã‚„ã™ã„ï¼‰
    # score = Wvãƒ»v_total
    scores = W_char_vow @ v_total  # (12,)
    h = -scores.copy()

    # äºŒæ¬¡é …ï¼šã‚­ãƒ£ãƒ©é–“ç›¸æ€§ï¼ˆè¿‘ã„ã»ã©ï¼ˆåŒæ™‚ã«ç«‹ã¡ã‚„ã™ã„ï¼ç«‹ã¡ã«ãã„ï¼‰ï¼‰
    # ã“ã“ã§ã¯ã€Œä¼¼ã¦ã‚‹åŒå£«ã¯åŒæ™‚ã«é¸ã¶ã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä¸ŠãŒã‚‹ã€= ç«¶åˆã¨ã—ã¦ +sim
    # â†’ 12ç¥ãŒåƒ…å·®ã§æºã‚‰ãã¨ã€åˆ†å¸ƒãŒâ€œåœ°å½¢ã£ã½ãâ€ãªã‚‹
    Wn = W_char_vow.copy().astype(float)
    # æ­£è¦åŒ–
    norms = np.linalg.norm(Wn, axis=1, keepdims=True) + 1e-12
    Wn = Wn / norms
    sim = Wn @ Wn.T  # cosine
    J = beta_pair * sim
    np.fill_diagonal(J, 0.0)

    return QuboModel(h=h, J=J, lam=float(lam), target_k=int(target_k))

def qubo_energy(x: np.ndarray, model: QuboModel) -> float:
    x = x.astype(float)
    # linear
    e = float(np.dot(model.h, x))
    # quadratic (i<j)
    # x^T J x /2 ã ãŒå¯¾è§’0ã«ã—ã¦å¯¾ç§°ãªã®ã§åŠåˆ†
    e += 0.5 * float(x @ model.J @ x)
    # constraint
    s = float(np.sum(x))
    e += model.lam * (s - model.target_k) ** 2
    return e

def sa_sample(model: QuboModel, n_steps: int = 300, t0: float = 2.0, t1: float = 0.3) -> np.ndarray:
    """ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹SAã§1ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ12æ¬¡å…ƒäºŒå€¤ï¼‰"""
    d = model.h.shape[0]
    x = (np.random.rand(d) < 0.3).astype(int)

    # 0ãƒ™ã‚¯ãƒˆãƒ«å¯¾ç­–ï¼šæœ€ä½1å€‹ã¯ç«‹ã¦ã¦é–‹å§‹
    if x.sum() == 0:
        x[np.random.randint(0, d)] = 1

    e = qubo_energy(x, model)

    for step in range(n_steps):
        # æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆç·šå½¢ï¼‰
        t = t0 + (t1 - t0) * (step / max(1, n_steps - 1))
        i = np.random.randint(0, d)
        x2 = x.copy()
        x2[i] = 1 - x2[i]
        e2 = qubo_energy(x2, model)
        de = e2 - e
        if de <= 0:
            x, e = x2, e2
        else:
            p = math.exp(-de / max(1e-9, t))
            if np.random.rand() < p:
                x, e = x2, e2

    # æœ€å¾Œã«ã€Œå…¨éƒ¨0ã€ã‚’é˜²ã
    if x.sum() == 0:
        x[np.random.randint(0, d)] = 1
    return x.astype(int)

def sample_distribution(
    model: QuboModel,
    n_samples: int = 200,
    n_steps: int = 300,
    t0: float = 2.0,
    t1: float = 0.3,
) -> Tuple[np.ndarray, List[np.ndarray]]:
    """
    è¿”ã‚Šå€¤:
      counts: (12,) å„ç¥ã®å‡ºç¾å›æ•°ï¼ˆx_i==1ã®å›æ•°ï¼‰
      samples: å„ã‚µãƒ³ãƒ—ãƒ«ã®x
    """
    d = model.h.shape[0]
    counts = np.zeros(d, dtype=int)
    samples: List[np.ndarray] = []
    for _ in range(n_samples):
        x = sa_sample(model, n_steps=n_steps, t0=t0, t1=t1)
        counts += x
        samples.append(x)
    return counts, samples


# -----------------------------
# ç¥è¨—æ–‡ï¼ˆVOW_DICT + CHAR_MASTER + QUOTESï¼‰
# -----------------------------
def build_oracle_text(
    char_row: pd.Series,
    vow_dict: pd.DataFrame,
    v_total: np.ndarray,
    v_slider: np.ndarray,
    v_text: np.ndarray,
    quotes_df: Optional[pd.DataFrame],
    temperature: float,
) -> str:
    # VOWè¾æ›¸
    vcols = [f"VOW_{i:02d}" for i in range(1, 13)]
    # top vows
    top_idx = np.argsort(-v_total)[:3]
    lines = []

    char_name = str(char_row.get("å…¬å¼ã‚­ãƒ£ãƒ©å", char_row.get("CHAR_NAME", char_row.get("NAME", "ï¼ˆä¸æ˜ï¼‰"))))
    role = str(char_row.get("å½¹å‰²", "")).strip()
    role_note = str(char_row.get("å½¹å‰²è£œè¶³èª¬æ˜", "")).strip()
    ema = str(char_row.get("çµµé¦¬æ–‡å­—åˆ†æ", "")).strip()

    lines.append(f"### ğŸ”® ç¥è¨—ï¼š{char_name}")
    if role:
        lines.append(f"- **å½¹å‰²**ï¼š{role}")
    if role_note:
        lines.append(f"- **è£œè¶³**ï¼š{role_note}")
    if ema:
        lines.append(f"- **èª­ã¿**ï¼š{ema}")

    lines.append("")
    lines.append("#### ğŸ§­ ã‚ãªãŸã®èª“é¡˜ã®â€œæ ¸å¿ƒâ€ï¼ˆä¸Šä½3ã¤ï¼‰")

    # vow_dict ã® TITLE/SUBTITLE/UI_HINT ã‚’æ·»ãˆã‚‹
    # VOW_ID ã¯ 1..12 or VOW_01.. ãªã©æºã‚Œã‚‹å¯èƒ½æ€§ â†’ indexã§å¯¾å¿œ
    for k in top_idx:
        vow_no = k + 1
        # vow_dict ã®è¡Œã‚’æ¢ã™
        row = None
        if "VOW_ID" in vow_dict.columns:
            # æ•°å­—å«ã‚€ã‹ã§æ‹¾ã†
            for _, r in vow_dict.iterrows():
                s = str(r.get("VOW_ID", ""))
                m = re.search(r"(\d+)", s)
                if m and int(m.group(1)) == vow_no:
                    row = r
                    break
        if row is None and len(vow_dict) >= vow_no:
            # è¡Œé †ã«å…¥ã£ã¦ã„ã‚‹å ´åˆã®ä¿é™º
            row = vow_dict.iloc[vow_no - 1]

        label = str(row.get("LABEL", f"VOW_{vow_no:02d}")) if row is not None else f"VOW_{vow_no:02d}"
        title = str(row.get("TITLE", "")).strip() if row is not None else ""
        subtitle = str(row.get("SUBTITLE", "")).strip() if row is not None else ""
        hint = str(row.get("UI_HINT", "")).strip() if row is not None else ""

        val = float(v_total[k])
        val_s = float(v_slider[k])
        val_t = float(v_text[k])

        msg = f"- **{label}**ï¼ˆåˆç®— {val:.2f} / slider {val_s:.2f} / text {val_t:.2f}ï¼‰"
        if title:
            msg += f"ï¼š{title}"
        lines.append(msg)
        if subtitle:
            lines.append(f"  - {subtitle}")
        if hint:
            lines.append(f"  - *ãƒ’ãƒ³ãƒˆ*ï¼š{hint}")

    # QUOTESï¼ˆä»»æ„ï¼‰
    quote_line = pick_quote(quotes_df, top_idx=top_idx, temperature=temperature)
    if quote_line:
        lines.append("")
        lines.append("#### ğŸ•¯ï¸ æ·»ãˆã‚‰ã‚ŒãŸè¨€è‘‰")
        lines.append(f"> {quote_line}")

    lines.append("")
    lines.append("#### âœ… ä»Šæ—¥ã®ä¸€æ­©")
    lines.append("ä»Šã¯â€œæ­£è§£â€ã‚’æ¢ã™ã‚ˆã‚Šã€**èª“é¡˜ã®ä¸Šä½1ã¤ã ã‘**ã‚’å°ã•ãå®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„ã€‚è¦³æ¸¬ã¯ã€è¡Œå‹•ã§åæŸã—ã¦ã„ãã¾ã™ã€‚")

    return "\n".join(lines)

def pick_quote(quotes_df: Optional[pd.DataFrame], top_idx: np.ndarray, temperature: float) -> Optional[str]:
    if quotes_df is None or len(quotes_df) == 0:
        return None

    df = quotes_df.copy()
    cols = list(df.columns)

    text_col = _first_existing(["TEXT", "QUOTE", "æ ¼è¨€", "åè¨€", "BODY", "æ–‡ç« ", "è¨€è‘‰"], cols)
    author_col = _first_existing(["AUTHOR", "è‘—è€…", "å‡ºå…¸", "SOURCE"], cols)
    tag_col = _first_existing(["TAG", "TAGS", "VOW_ID", "VOW", "ã‚«ãƒ†ã‚´ãƒª", "CATEGORY"], cols)

    if not text_col:
        return None

    # å€™è£œæŠ½å‡ºï¼ˆTAGã« VOWç•ªå·ãŒå«ã¾ã‚Œã‚‹ãªã‚‰å¯„ã›ã‚‹ï¼‰
    candidates = []
    top_vows = [int(i + 1) for i in top_idx]

    if tag_col:
        for _, r in df.iterrows():
            t = str(r.get(tag_col, "")).strip()
            if not t:
                continue
            hit = False
            for vn in top_vows:
                if re.search(rf"\b{vn}\b", t) or re.search(rf"VOW[_\- ]*0*{vn}\b", t, flags=re.IGNORECASE):
                    hit = True
                    break
            if hit:
                candidates.append(r)

    # å€™è£œãŒå°‘ãªã‘ã‚Œã°å…¨ä½“ã‹ã‚‰
    if len(candidates) < 3:
        candidates = [r for _, r in df.iterrows()]

    if not candidates:
        return None

    # æ¸©åº¦ã§â€œå¯„ã›ã‚‹åº¦åˆã„â€ã‚’å¤‰ãˆã‚‹ï¼šä½æ¸©=ä¸Šä½ã‹ã‚‰ã€ é«˜æ¸©=ãƒ©ãƒ³ãƒ€ãƒ åºƒã‚
    # ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ã€æ¸©åº¦ãŒä½ã„ã»ã©å…ˆé ­è¿‘ãã‚’é¸ã³ã‚„ã™ãã™ã‚‹é‡ã¿
    n = len(candidates)
    ranks = np.arange(n, dtype=float)
    # æ¸©åº¦ãŒä½ã„ã»ã©æ¸›è¡°ã‚’å¼·ã
    tau = max(0.25, float(temperature))
    weights = np.exp(-ranks / (2.0 * tau))
    weights = weights / (weights.sum() + 1e-12)
    idx = int(np.random.choice(np.arange(n), p=weights))
    r = candidates[idx]

    q = str(r.get(text_col, "")).strip()
    if not q:
        return None
    a = str(r.get(author_col, "")).strip() if author_col else ""
    return f"{q}" + (f"ï¼ˆ{a}ï¼‰" if a else "")


# -----------------------------
# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰ï¼ˆåˆ—åã‚†ã‚‰ãå¸åï¼‰
# -----------------------------
@dataclass
class CharTable:
    ids: List[str]
    names: List[str]
    images: List[str]         # ãƒ•ã‚¡ã‚¤ãƒ«å or ãƒ‘ã‚¹
    W_char_vow: np.ndarray    # (12,12)
    master_df: pd.DataFrame   # CHAR_MASTERå‚ç…§ç”¨ï¼ˆè¡Œå¼•ãï¼‰

def build_char_table(pack: PackData) -> CharTable:
    ctv = pack.char_to_vow.copy()
    cm = pack.char_master.copy()

    # åˆ—åå€™è£œ
    ctv_cols = list(ctv.columns)
    id_col = _first_existing(["CHAR_ID", "ID", "ã‚­ãƒ£ãƒ©ID"], ctv_cols)
    name_col = _first_existing(["å…¬å¼ã‚­ãƒ£ãƒ©å", "CHAR_NAME", "NAME", "ã‚­ãƒ£ãƒ©å"], ctv_cols)
    img_col = _first_existing(["IMAGE_FILE", "IMAGE", "IMG", "ç”»åƒ", "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«"], ctv_cols)

    # VOWåˆ—
    vow_cols = _ensure_vow_cols(ctv, 12)

    # å¿…é ˆï¼ˆid/nameã¯ç‰‡æ–¹æ¬ ã‘ã¦ã‚‚ã€æœ€æ‚ª index ã§è£œã†ï¼‰
    if id_col is None:
        # CHAR_MASTERå´ã§æ‹¾ãˆã‚‹ãªã‚‰æ‹¾ã†
        if "CHAR_ID" in cm.columns:
            ctv["CHAR_ID"] = cm["CHAR_ID"].values[:len(ctv)]
            id_col = "CHAR_ID"
        else:
            ctv["CHAR_ID"] = [f"CHAR_{i+1:02d}" for i in range(len(ctv))]
            id_col = "CHAR_ID"

    if name_col is None:
        # CHAR_MASTERã®å…¬å¼ã‚­ãƒ£ãƒ©åã§è£œå®Œ
        cm_cols = list(cm.columns)
        cm_name_col = _first_existing(["å…¬å¼ã‚­ãƒ£ãƒ©å", "CHAR_NAME", "NAME", "ã‚­ãƒ£ãƒ©å"], cm_cols)
        if cm_name_col is not None:
            # CHAR_IDã§ãƒãƒ¼ã‚¸
            ctv = ctv.merge(cm[[ "CHAR_ID", cm_name_col ]], on="CHAR_ID", how="left", suffixes=("", "_m"))
            ctv["å…¬å¼ã‚­ãƒ£ãƒ©å"] = ctv["å…¬å¼ã‚­ãƒ£ãƒ©å"] if "å…¬å¼ã‚­ãƒ£ãƒ©å" in ctv.columns else ctv[cm_name_col]
            name_col = "å…¬å¼ã‚­ãƒ£ãƒ©å"
        else:
            ctv["å…¬å¼ã‚­ãƒ£ãƒ©å"] = ctv[id_col].astype(str)
            name_col = "å…¬å¼ã‚­ãƒ£ãƒ©å"

    if img_col is None:
        # CHAR_TO_VOWã«ç„¡ã„å ´åˆã¯ç©ºã§OKï¼ˆç”»åƒã¯ä»»æ„ï¼‰
        ctv["IMAGE_FILE"] = ""
        img_col = "IMAGE_FILE"

    ids = [str(x) for x in ctv[id_col].tolist()]
    names = [str(x) for x in ctv[name_col].tolist()]
    images = [str(x) for x in ctv[img_col].fillna("").tolist()]
    W = np.array(ctv[vow_cols].fillna(0.0).astype(float).values, dtype=float)

    # 12ç¥ã«åˆã‚ã›ã¦åˆ‡ã‚Šè©°ã‚/è£œã†ï¼ˆå¿µã®ãŸã‚ï¼‰
    if W.shape[0] < 12:
        pad = np.zeros((12 - W.shape[0], 12), dtype=float)
        W = np.vstack([W, pad])
        ids += [f"CHAR_PAD_{i+1}" for i in range(12 - len(ids))]
        names += [f"ï¼ˆæœªå®šç¾©ï¼‰{i+1}" for i in range(12 - len(names))]
        images += [""] * (12 - len(images))
    if W.shape[0] > 12:
        W = W[:12, :]
        ids = ids[:12]
        names = names[:12]
        images = images[:12]

    return CharTable(ids=ids, names=names, images=images, W_char_vow=W, master_df=cm)


# -----------------------------
# UI
# -----------------------------
st.sidebar.title("ğŸ“ ãƒ‡ãƒ¼ã‚¿")

pack_file = st.sidebar.file_uploader(
    "çµ±åˆExcelï¼ˆpackï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆæœ€å„ªå…ˆï¼‰",
    type=["xlsx"],
    help="ä¾‹ï¼šquantum_shintaku_pack_v3_with_sense_*.xlsx",
)

st.sidebar.markdown("---")
img_dir = st.sidebar.text_input(
    "ğŸ–¼ï¸ ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ï¼ˆç›¸å¯¾/çµ¶å¯¾ï¼‰",
    value="./assets/images/characters/",
    help="ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼š./assets/images/characters/  ä¾‹ï¼šC:\\Users\\...\\assets\\images\\characters",
)

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸŒ¡ï¸ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š")
temperature = st.sidebar.slider("æ¸©åº¦ Tï¼ˆé«˜ã„ã»ã©æºã‚‰ãï¼‰", 0.2, 3.0, 1.1, 0.1)
n_samples = st.sidebar.slider("ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆåˆ†å¸ƒç”¨ï¼‰", 50, 800, 250, 10)
sa_steps = st.sidebar.slider("SAã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆ1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Šï¼‰", 80, 1200, 350, 10)
beta_pair = st.sidebar.slider("ç›¸æ€§ï¼ˆäºŒæ¬¡é …ï¼‰å¼·ã• Î²", 0.0, 0.6, 0.18, 0.01)
lam = st.sidebar.slider("åˆ¶ç´„ãƒšãƒŠãƒ«ãƒ†ã‚£ Î»ï¼ˆç›®æ¨™=1æŸ±ï¼‰", 0.5, 12.0, 3.5, 0.1)

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ§ª å…¥åŠ›çµ±åˆ")
w_slider = st.sidebar.slider("w1: ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼èª“é¡˜ã®é‡ã¿", 0.0, 3.0, 1.0, 0.05)
w_text = st.sidebar.slider("w2: ãƒ†ã‚­ã‚¹ãƒˆèª“é¡˜ã®é‡ã¿", 0.0, 3.0, 1.1, 0.05)

st.title("ğŸ”® Q-Quest é‡å­ç¥è¨— app08ï¼ˆå®Œæˆç‰ˆï¼‰")
st.caption("èª“é¡˜ï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ï¼‹æ–‡ç« ï¼‰â†’ QUBO â†’ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åˆ†å¸ƒ â†’ ä»Šå›â€œè¦³æ¸¬â€ã•ã‚ŒãŸç¥ã¨ç¥è¨—ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")

if not pack_file:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ **çµ±åˆExcelï¼ˆpackï¼‰** ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# èª­ã¿è¾¼ã¿
try:
    pack = load_pack_excel(pack_file)
except Exception as e:
    st.error(f"çµ±åˆExcelã®è§£æã«å¤±æ•—: {e}")
    st.stop()

# è¡¨ç¤ºï¼ˆæ¤œå‡ºæƒ…å ±ï¼‰
with st.expander("ğŸ” æ¤œå‡ºã—ãŸã‚·ãƒ¼ãƒˆå / åˆ—åï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰", expanded=False):
    st.write("æ¤œå‡ºã—ãŸã‚·ãƒ¼ãƒˆå:", pack.sheet_names)
    for nm in ["VOW_DICT", "CHAR_MASTER", "CHAR_TO_VOW", "SENSE_TO_VOW", "QUOTES"]:
        df = pack.sheets.get(nm, None)
        if df is not None:
            st.write(f"**{nm} åˆ—å:**", list(df.columns))

# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
try:
    char_table = build_char_table(pack)
except Exception as e:
    st.error(f"ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆã«å¤±æ•—: {e}")
    st.stop()

# SENSEãƒãƒƒãƒ—
sense_label_map, sense2vow_vec = build_sense_maps(pack.sense_dict, pack.sense_to_vow)

# VOWè¾æ›¸ï¼ˆ12å€‹ã ã‘ä½¿ã†ï¼‰
vow_dict = pack.vow_dict.copy()

# UI: å…¥åŠ›
colL, colR = st.columns([1.05, 0.95], gap="large")

with colL:
    st.subheader("âœ… Step 1ï¼šèª“é¡˜å…¥åŠ›ï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ï¼‰")
    st.write("å„èª“é¡˜ã‚’ 0ã€œ5 ã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆæ˜ç¤ºæ„å›³ï¼‰ã€‚")

    # VOWè¡¨ç¤ºãƒ©ãƒ™ãƒ«
    vow_labels: List[str] = []
    if "LABEL" in vow_dict.columns and len(vow_dict) >= 12:
        vow_labels = [str(vow_dict.iloc[i].get("LABEL", f"VOW_{i+1:02d}")) for i in range(12)]
    else:
        vow_labels = [f"VOW_{i+1:02d}" for i in range(12)]

    sliders = []
    for i in range(12):
        # ã‚¿ã‚¤ãƒˆãƒ«ã€èª¬æ˜ã€ãƒ’ãƒ³ãƒˆã‚’å–å¾—
        title = ""
        description = ""
        hint = ""
        if len(vow_dict) > i:
            row = vow_dict.iloc[i]
            if "TITLE" in vow_dict.columns:
                title = str(row.get("TITLE", "")).strip()
            if "DESCRIPTION_LONG" in vow_dict.columns:
                description = str(row.get("DESCRIPTION_LONG", "")).strip()
            elif "DESCRIPTION" in vow_dict.columns:
                description = str(row.get("DESCRIPTION", "")).strip()
            if "UI_HINT" in vow_dict.columns:
                hint = str(row.get("UI_HINT", "")).strip()
        
        # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ãƒ©ãƒ™ãƒ«ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’å«ã‚ã‚‹ï¼ˆã‚ã‚Œã°ï¼‰
        slider_label = vow_labels[i]
        if title:
            slider_label = f"{vow_labels[i]}ï¼š{title}"
        
        val = st.slider(slider_label, 0, 5, 0, 1)
        sliders.append(val)
        
        # èª¬æ˜ã¨ãƒ’ãƒ³ãƒˆã‚’å¸¸æ™‚è¡¨ç¤ºï¼ˆã‚ã‚Œã°ï¼‰
        if description or hint:
            info_text = []
            if description:
                info_text.append(description)
            if hint:
                info_text.append(f"ğŸ’¡ {hint}")
            if info_text:
                st.caption(" | ".join(info_text))

    v_slider = np.array(sliders, dtype=float)
    # æ­£è¦åŒ–ï¼ˆ0-5â†’0-1ï¼‰
    v_slider_n = v_slider / 5.0

    st.subheader("ğŸ“ Step 1bï¼šèª“é¡˜å…¥åŠ›ï¼ˆæ–‡ç« ï¼‰")
    text = st.text_area(
        "ã‚ãªãŸã®èª“é¡˜ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ï¼ˆæš—é»™æ„å›³ã‚’æŠ½å‡ºã—ã¾ã™ï¼‰",
        value="",
        height=120,
        placeholder="ä¾‹ï¼šè¿·ã„ã‚’æ–­ã¡åˆ‡ã£ã¦ä¸€æ­©è¸ã¿å‡ºã—ãŸã„ã€‚è‡ªåˆ†ã®èŠ¯ã‚’å–ã‚Šæˆ»ã—ãŸã„ã€‚"
    )

    v_text, hits = text_to_vow_vector(text, sense_label_map, sense2vow_vec, ngram_n=3)
    # v_text ã¯æ­£è¦åŒ–æ¸ˆã¿ï¼ˆ0..1ç¨‹åº¦ï¼‰ãªã®ã§ã€åˆã‚ã›ã¦ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒãˆã‚‹
    v_text_n = _norm01(v_text)

    if hits:
        with st.expander("ğŸ§© ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¤œå‡ºã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆSENSEï¼‰", expanded=False):
            st.write(pd.DataFrame(hits, columns=["SENSE", "ä¸€è‡´ã‚¹ã‚³ã‚¢"]).head(12))

    # åˆç®—
    v_total = w_slider * v_slider_n + w_text * v_text_n
    # è¦‹æ „ãˆç”¨ã«0-1ã¸
    v_total_n = _norm01(v_total)

with colR:
    st.subheader("ğŸ“Œ å½±éŸ¿ã®å¯è¦–åŒ–ï¼ˆå…¥åŠ›ãŒã©ã†åŠ¹ã„ãŸã‹ï¼‰")
    df_vis = pd.DataFrame({
        "VOW": vow_labels,
        "slider": v_slider_n,
        "text": v_text_n,
        "total": v_total_n
    }).set_index("VOW")

    st.write("**VOWãƒ™ã‚¯ãƒˆãƒ«ï¼ˆslider / text / totalï¼‰**")
    st.bar_chart(df_vis[["slider", "text", "total"]])

    # Topå¯„ä¸
    top_df = df_vis.copy()
    top_df["total_rank"] = (-top_df["total"]).rank(method="first")
    top_df = top_df.sort_values("total", ascending=False).head(6)
    st.write("**å¯„ä¸Topï¼ˆä¸Šä½6ï¼‰**")
    st.dataframe(top_df[["slider", "text", "total"]], use_container_width=True)

# QUBOæ§‹ç¯‰
W = char_table.W_char_vow  # (12,12)
model = build_qubo(W_char_vow=W, v_total=v_total_n, beta_pair=beta_pair, lam=lam, target_k=1)

# ã‚­ãƒ£ãƒ©ä¸€æ¬¡ã‚¹ã‚³ã‚¢ï¼ˆå‚è€ƒï¼‰
char_scores = W @ v_total_n  # é«˜ã„ã»ã©åˆã†
char_energies_unary = -char_scores  # å°ã•ã„ã»ã©åˆã†

# è¦³æ¸¬
st.markdown("---")
st.subheader("âœ… Step 2ï¼šQUBOã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆè¦³æ¸¬ï¼‰")

obs_col1, obs_col2 = st.columns([0.55, 0.45], gap="large")
with obs_col1:
    st.write("**è¦³æ¸¬ãƒœã‚¿ãƒ³**ã‚’æŠ¼ã™ãŸã³ã«ã€åŒã˜æ¡ä»¶ã§ã‚‚â€œæºã‚‰ãâ€ã«ã‚ˆã‚ŠçµæœãŒå¤‰ã‚ã‚Šå¾—ã¾ã™ã€‚")
    observe = st.button("ğŸ‘ï¸ è¦³æ¸¬ã™ã‚‹ï¼ˆQUBOã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã¦ç¥ã‚’è¦³æ¸¬ï¼‰", use_container_width=True)

with obs_col2:
    st.info(
        "â„¹ï¸ **ã€ä»Šå›è¦³æ¸¬ã•ã‚ŒãŸç¥ã€ã¨ã€åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ Topã€ãŒã‚ºãƒ¬ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚**\n\n"
        "- è¦³æ¸¬ï¼š**1å›ã‚µãƒ³ãƒ—ãƒ«**ï¼ˆä¹±æ•°/æ¸©åº¦ã®å½±éŸ¿ã‚’å¼·ãå—ã‘ã‚‹ï¼‰\n"
        "- ãƒ’ã‚¹ãƒˆï¼š**å¤šæ•°å›ã‚µãƒ³ãƒ—ãƒ«ã®çµ±è¨ˆ**\n\n"
        "ä¸Šä½å€™è£œãŒæ‹®æŠ—ã—ã¦ã„ã‚‹ã»ã©ã€1å›è¦³æ¸¬ã¯Topä»¥å¤–ã«ã‚‚é£›ã³ã¾ã™ã€‚"
    )

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿æŒï¼ˆæœ€å¾Œã«è¦³æ¸¬ã•ã‚ŒãŸç¥ï¼‰
if "last_obs_idx" not in st.session_state:
    st.session_state.last_obs_idx = None
if "last_samples_counts" not in st.session_state:
    st.session_state.last_samples_counts = None

if observe:
    counts, samples = sample_distribution(
        model=model,
        n_samples=int(n_samples),
        n_steps=int(sa_steps),
        t0=float(temperature * 2.0),
        t1=float(temperature * 0.35),
    )
    st.session_state.last_samples_counts = counts

    # â€œä»Šå›ã®è¦³æ¸¬â€ï¼šåˆ†å¸ƒã‹ã‚‰ç¢ºç‡çš„ã«1æŸ±ã‚’é¸ã¶ï¼ˆãƒãƒ¼ã‚¸ãƒŠãƒ«ã‚’åˆ©ç”¨ï¼‰
    # counts ã¯ã€Œãã®ç¥ãŒç«‹ã£ãŸå›æ•°ã€ãªã®ã§ç¢ºç‡ã«ã—ã¦æŠ½é¸
    probs = counts.astype(float)
    if probs.sum() <= 0:
        probs = np.ones(12, dtype=float)
    probs = probs / probs.sum()

    obs_idx = int(np.random.choice(np.arange(12), p=probs))
    st.session_state.last_obs_idx = obs_idx

# è¡¨ç¤º
counts = st.session_state.last_samples_counts
obs_idx = st.session_state.last_obs_idx

if counts is None or obs_idx is None:
    st.warning("ã¾ã è¦³æ¸¬ã—ã¦ã„ã¾ã›ã‚“ã€‚ä¸Šã® **ğŸ‘ï¸ è¦³æ¸¬ã™ã‚‹** ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆï¼‰
st.subheader("ğŸ“Š è¦³æ¸¬åˆ†å¸ƒï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰")
df_hist = pd.DataFrame({"ç¥": char_table.names, "count": counts}).set_index("ç¥")
st.bar_chart(df_hist)

# è¦³æ¸¬çµæœï¼ˆã‚­ãƒ£ãƒ©è¡¨ç¤ºï¼‰
st.markdown("---")
st.subheader("ğŸ§¿ ä»Šå›â€œè¦³æ¸¬â€ã•ã‚ŒãŸç¥ï¼ˆåŸºæ¿æ›¼è¼ç¾…ï¼‰")

# CHAR_MASTERã‹ã‚‰è©²å½“è¡Œã‚’æ¢ã™ï¼ˆCHAR_IDä¸€è‡´ï¼‰
cm = char_table.master_df.copy()
cm_id_col = "CHAR_ID" if "CHAR_ID" in cm.columns else None
row = None
if cm_id_col:
    # CHAR_TO_VOWå´ã®IDã§çªåˆ
    cid = char_table.ids[obs_idx]
    m = cm[cm[cm_id_col].astype(str) == str(cid)]
    if len(m) > 0:
        row = m.iloc[0]
if row is None:
    # fallback: åå‰ä¸€è‡´
    name = char_table.names[obs_idx]
    name_col = _first_existing(["å…¬å¼ã‚­ãƒ£ãƒ©å", "CHAR_NAME", "NAME", "ã‚­ãƒ£ãƒ©å"], list(cm.columns))
    if name_col:
        m = cm[cm[name_col].astype(str) == str(name)]
        if len(m) > 0:
            row = m.iloc[0]
if row is None:
    # æœ€å¾Œã®æ‰‹æ®µï¼šç©ºã®Series
    row = pd.Series({"å…¬å¼ã‚­ãƒ£ãƒ©å": char_table.names[obs_idx]})

# ç”»åƒ
img_file = char_table.images[obs_idx]
img_path = ""
if img_file:
    # ã™ã§ã«ãƒ•ãƒ«ãƒ‘ã‚¹ãªã‚‰ãã®ã¾ã¾ã€ãƒ•ã‚¡ã‚¤ãƒ«åãªã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµåˆ
    if os.path.isabs(img_file) and os.path.exists(img_file):
        img_path = img_file
    else:
        img_path = os.path.join(img_dir, img_file)

img = _read_image_maybe(img_path)

left, right = st.columns([0.42, 0.58], gap="large")
with left:
    if img is not None:
        st.image(img, caption=f"{char_table.names[obs_idx]}ï¼ˆ{img_file}ï¼‰", use_container_width=True)
    else:
        st.warning(
            "ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\n"
            f"- æœŸå¾…ãƒ‘ã‚¹: `{img_path}`\n"
            "- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ç”»åƒãƒ•ã‚©ãƒ«ãƒ€è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
            "- CHAR_TO_VOW ã® IMAGE_FILE ãŒç©ºã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚"
        )

with right:
    # ç¥è¨—æ–‡
    oracle = build_oracle_text(
        char_row=row,
        vow_dict=vow_dict,
        v_total=v_total_n,
        v_slider=v_slider_n,
        v_text=v_text_n,
        quotes_df=pack.quotes,
        temperature=float(temperature),
    )
    st.markdown(oracle)

# è¿½åŠ ï¼šä¸Šä½å€™è£œï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼é †ä½ï¼‰
st.markdown("---")
st.subheader("ğŸ ã‚¨ãƒãƒ«ã‚®ãƒ¼é †ä½ï¼ˆå‚è€ƒï¼šä¸€æ¬¡é …ãƒ™ãƒ¼ã‚¹ï¼‰")
rank_idx = np.argsort(char_energies_unary)[:5]
rank_df = pd.DataFrame({
    "é †ä½": np.arange(1, len(rank_idx) + 1),
    "ç¥": [char_table.names[i] for i in rank_idx],
    "energy(unary)": [float(char_energies_unary[i]) for i in rank_idx],
    "score": [float(char_scores[i]) for i in rank_idx],
})
st.dataframe(rank_df, use_container_width=True)

st.caption(
    "â€» ä¸Šã®é †ä½ã¯ä¸»ã«ã€èª“é¡˜ãƒ™ã‚¯ãƒˆãƒ«ã¨ã®æ•´åˆï¼ˆä¸€æ¬¡é …ï¼‰ã€ã®å‚è€ƒã§ã™ã€‚"
    " å®Ÿéš›ã®è¦³æ¸¬ã¯ QUBO + SA ã®æºã‚‰ãï¼ˆæ¸©åº¦Tã€ç›¸æ€§Î²ã€åˆ¶ç´„Î»ï¼‰ã«ã‚ˆã‚Šå¤‰å‹•ã—ã¾ã™ã€‚"
)
